name: Daily Property Scrape

on:
  schedule:
    # Run daily at 6 AM UTC (7 AM UK time in winter, 8 AM in summer)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      spiders:
        description: 'Spiders to run (comma-separated or "all")'
        required: false
        default: 'all'
      full_mode:
        description: 'Run with --full (fetch details + floorplans)'
        required: false
        default: 'true'
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  # Vercel Postgres connection (set in GitHub Secrets)
  POSTGRES_URL: ${{ secrets.POSTGRES_URL }}

permissions:
  contents: write
  issues: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout (scrape takes ~2 hours)

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          pip install playwright
          playwright install chromium
          playwright install-deps chromium

      - name: Check Postgres connection
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("ERROR: POSTGRES_URL not set!")
              exit(1)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()
              cursor.execute("SELECT COUNT(*) FROM listings")
              count = cursor.fetchone()[0]
              print(f"Connected to Vercel Postgres. Current listings: {count}")
              conn.close()
          except Exception as e:
              print(f"Database connection failed: {e}")
              print("Will create tables on first scrape")
          EOF

      - name: Run scraper (Postgres mode)
        id: scrape
        env:
          # Use Postgres settings for all spiders
          USE_POSTGRES: "true"
        run: |
          # Determine spider arguments
          SPIDER_ARG=""
          if [ "${{ github.event.inputs.spiders }}" != "" ] && [ "${{ github.event.inputs.spiders }}" != "all" ]; then
            IFS=',' read -ra SPIDERS <<< "${{ github.event.inputs.spiders }}"
            for spider in "${SPIDERS[@]}"; do
              SPIDER_ARG="$SPIDER_ARG -s $(echo $spider | xargs)"
            done
          else
            SPIDER_ARG="--all"
          fi

          # Determine full mode
          FULL_FLAG=""
          if [ "${{ github.event.inputs.full_mode }}" = "true" ] || [ "${{ github.event_name }}" = "schedule" ]; then
            FULL_FLAG="--full"
          fi

          # Run the scrape with Postgres settings
          echo "Running: python -m cli.main scrape $SPIDER_ARG $FULL_FLAG --postgres"
          python -m cli.main scrape $SPIDER_ARG $FULL_FLAG --postgres 2>&1 | tee scrape_output.txt

          # Extract summary for GitHub output
          ITEMS=$(grep -oP 'Total: \K[\d,]+' scrape_output.txt | tail -1 || echo "0")
          echo "items_scraped=$ITEMS" >> $GITHUB_OUTPUT

      - name: Mark stale listings inactive
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              # Mark listings not seen in 2+ days as inactive
              cursor.execute("""
                  UPDATE listings
                  SET is_active = 0
                  WHERE last_seen::timestamp < NOW() - INTERVAL '2 days'
                  AND is_active = 1
              """)
              marked = cursor.rowcount
              conn.commit()

              if marked > 0:
                  print(f"Marked {marked} stale listings as inactive")
              else:
                  print("No stale listings to mark inactive")

              conn.close()
          except Exception as e:
              print(f"Error marking inactive: {e}")
          EOF

      - name: Show database stats
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("POSTGRES_URL not set")
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              print("=== Post-scrape database stats ===")
              cursor.execute("""
                  SELECT source, COUNT(*) as total,
                         SUM(CASE WHEN is_active = 1 THEN 1 ELSE 0 END) as active,
                         SUM(CASE WHEN size_sqft > 0 THEN 1 ELSE 0 END) as with_sqft
                  FROM listings GROUP BY source ORDER BY total DESC
              """)
              for row in cursor.fetchall():
                  print(f"  {row[0]}: {row[1]} total, {row[2]} active, {row[3]} with sqft")

              cursor.execute("SELECT COUNT(*) FROM listings")
              print(f"\nTotal listings: {cursor.fetchone()[0]}")

              cursor.execute("SELECT COUNT(*) FROM listings WHERE size_sqft > 0")
              print(f"With sqft: {cursor.fetchone()[0]}")

              conn.close()
          except Exception as e:
              print(f"Error querying database: {e}")
          EOF

      - name: Show recent scrape runs
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              print("=== Recent Scrape Runs ===")
              cursor.execute("""
                  SELECT run_id, spider_name, status, items_scraped, error_count,
                         ROUND(duration_seconds/60, 1) as minutes
                  FROM scrape_runs
                  ORDER BY started_at DESC
                  LIMIT 10
              """)
              for row in cursor.fetchall():
                  status_icon = "✓" if row[2] == "completed" else "✗"
                  print(f"  {status_icon} {row[0]} | {row[1]}: {row[3]} items, {row[4]} errors ({row[5]}m)")

              conn.close()
          except Exception as e:
              print(f"Error: {e}")
          EOF

      - name: Clean duplicate listings
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("POSTGRES_URL not set")
              exit(0)

          conn = psycopg2.connect(url)
          cursor = conn.cursor()

          # Find duplicates to delete (keep best source per price/size/beds/district cluster)
          cursor.execute("""
              WITH ranked AS (
                  SELECT
                      id,
                      ROW_NUMBER() OVER (
                          PARTITION BY price_pcm, size_sqft, bedrooms, SPLIT_PART(postcode, ' ', 1)
                          ORDER BY
                              CASE source
                                  WHEN 'savills' THEN 1 WHEN 'knightfrank' THEN 2
                                  WHEN 'chestertons' THEN 3 WHEN 'foxtons' THEN 4
                                  ELSE 5
                              END, id
                      ) as rn,
                      COUNT(*) OVER (PARTITION BY price_pcm, size_sqft, bedrooms, SPLIT_PART(postcode, ' ', 1)) as cnt
                  FROM listings
                  WHERE is_active = 1 AND size_sqft > 0 AND price_pcm > 0
              )
              SELECT id FROM ranked WHERE cnt > 1 AND rn > 1
          """)
          to_delete = [row[0] for row in cursor.fetchall()]

          if to_delete:
              # Delete price_history first (FK constraint)
              placeholders = ','.join(['%s'] * len(to_delete))
              cursor.execute(f"DELETE FROM price_history WHERE listing_id IN ({placeholders})", to_delete)
              cursor.execute(f"DELETE FROM listings WHERE id IN ({placeholders})", to_delete)
              conn.commit()
              print(f"✅ Removed {len(to_delete)} duplicate listings")
          else:
              print("✅ No duplicates found")

          conn.close()
          EOF

      - name: Train price prediction model (V15)
        run: |
          pip install xgboost scikit-learn optuna pandas numpy
          # Full training for scheduled runs, quick mode for manual tests
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "Running FULL model training (Optuna optimization)..."
            python rental_price_models_v15.py 2>&1 | tee model_output.txt
          else
            echo "Running QUICK model training (skip Optuna)..."
            python rental_price_models_v15.py --quick 2>&1 | tee model_output.txt
          fi
        timeout-minutes: 30

      - name: Show model results
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              cursor.execute("""
                  SELECT run_id, version, samples_total, r2_score, mae, mape, median_ape
                  FROM model_runs ORDER BY created_at DESC LIMIT 1
              """)
              row = cursor.fetchone()
              if row:
                  print(f"=== Latest Model: {row[0]} ({row[1]}) ===")
                  print(f"  Samples: {row[2]}")
                  print(f"  R²: {row[3]:.4f}")
                  print(f"  MAE: £{row[4]:,.0f}")
                  print(f"  MAPE: {row[5]:.1f}%")
                  print(f"  Median APE: {row[6]:.1f}%")

              conn.close()
          except Exception as e:
              print(f"Error: {e}")
          EOF

      # Note: Negotiation report is now generated dynamically at /report
      # No static files needed - data is fetched fresh from Postgres on each load

      - name: Upload logs artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scrape-logs-${{ github.run_id }}
          path: |
            logs/*.log
            scrape_output.txt
            model_output.txt
          retention-days: 7

      - name: Create summary
        run: |
          echo "## Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Items scraped:** ${{ steps.scrape.outputs.items_scraped }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Database:** Vercel Postgres" >> $GITHUB_STEP_SUMMARY
          echo "- **Dashboard:** https://dashboard-fawn-nu-59.vercel.app" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("Database not configured")
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              print("### Database Stats")
              print("| Source | Total | Active | With sqft |")
              print("|--------|-------|--------|-----------|")

              cursor.execute("""
                  SELECT source, COUNT(*),
                         SUM(CASE WHEN is_active = 1 THEN 1 ELSE 0 END),
                         SUM(CASE WHEN size_sqft > 0 THEN 1 ELSE 0 END)
                  FROM listings GROUP BY source ORDER BY COUNT(*) DESC
              """)
              for row in cursor.fetchall():
                  print(f"| {row[0]} | {row[1]} | {row[2]} | {row[3]} |")

              conn.close()
          except Exception as e:
              print(f"Error: {e}")
          EOF

  notify-failure:
    needs: scrape
    runs-on: ubuntu-latest
    if: failure()
    steps:
      - name: Create issue on failure
        uses: actions/github-script@v7
        with:
          script: |
            const title = `Scrape failed: ${new Date().toISOString().split('T')[0]}`;
            const body = `
            The daily scrape workflow failed.

            **Workflow run:** [${context.runId}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

            **Dashboard:** Check https://dashboard-fawn-nu-59.vercel.app for details.

            Please check the logs for details.
            `;

            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'scrape-failure'
            });

            if (issues.data.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['scrape-failure', 'automated']
              });
            }
