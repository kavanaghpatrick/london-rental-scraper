name: Daily Property Scrape

on:
  schedule:
    # Run daily at 6 AM UTC (7 AM UK time in winter, 8 AM in summer)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      spiders:
        description: 'Spiders to run (comma-separated or "all")'
        required: false
        default: 'all'
      full_mode:
        description: 'Run with --full (fetch details + floorplans)'
        required: false
        default: 'true'
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  # Vercel Postgres connection (set in GitHub Secrets)
  POSTGRES_URL: ${{ secrets.POSTGRES_URL }}

permissions:
  contents: write
  issues: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout (scrape takes ~2 hours)

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Playwright browsers
        run: |
          pip install playwright
          playwright install chromium
          playwright install-deps chromium

      - name: Check Postgres connection
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("ERROR: POSTGRES_URL not set!")
              exit(1)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()
              cursor.execute("SELECT COUNT(*) FROM listings")
              count = cursor.fetchone()[0]
              print(f"Connected to Vercel Postgres. Current listings: {count}")
              conn.close()
          except Exception as e:
              print(f"Database connection failed: {e}")
              print("Will create tables on first scrape")
          EOF

      - name: Run scraper (Postgres mode)
        id: scrape
        env:
          # Use Postgres settings for all spiders
          USE_POSTGRES: "true"
        run: |
          # Determine spider arguments
          SPIDER_ARG=""
          if [ "${{ github.event.inputs.spiders }}" != "" ] && [ "${{ github.event.inputs.spiders }}" != "all" ]; then
            IFS=',' read -ra SPIDERS <<< "${{ github.event.inputs.spiders }}"
            for spider in "${SPIDERS[@]}"; do
              SPIDER_ARG="$SPIDER_ARG -s $(echo $spider | xargs)"
            done
          else
            SPIDER_ARG="--all"
          fi

          # Determine full mode
          FULL_FLAG=""
          if [ "${{ github.event.inputs.full_mode }}" = "true" ] || [ "${{ github.event_name }}" = "schedule" ]; then
            FULL_FLAG="--full"
          fi

          # Run the scrape with Postgres settings
          echo "Running: python -m cli.main scrape $SPIDER_ARG $FULL_FLAG --postgres"
          python -m cli.main scrape $SPIDER_ARG $FULL_FLAG --postgres 2>&1 | tee scrape_output.txt

          # Extract summary for GitHub output
          ITEMS=$(grep -oP 'Total: \K[\d,]+' scrape_output.txt | tail -1 || echo "0")
          echo "items_scraped=$ITEMS" >> $GITHUB_OUTPUT

      - name: Mark stale listings inactive
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              # Mark listings not seen in 2+ days as inactive
              cursor.execute("""
                  UPDATE listings
                  SET is_active = 0
                  WHERE last_seen::timestamp < NOW() - INTERVAL '2 days'
                  AND is_active = 1
              """)
              marked = cursor.rowcount
              conn.commit()

              if marked > 0:
                  print(f"Marked {marked} stale listings as inactive")
              else:
                  print("No stale listings to mark inactive")

              conn.close()
          except Exception as e:
              print(f"Error marking inactive: {e}")
          EOF

      - name: Show database stats
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("POSTGRES_URL not set")
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              print("=== Post-scrape database stats ===")
              cursor.execute("""
                  SELECT source, COUNT(*) as total,
                         SUM(CASE WHEN is_active = 1 THEN 1 ELSE 0 END) as active,
                         SUM(CASE WHEN size_sqft > 0 THEN 1 ELSE 0 END) as with_sqft
                  FROM listings GROUP BY source ORDER BY total DESC
              """)
              for row in cursor.fetchall():
                  print(f"  {row[0]}: {row[1]} total, {row[2]} active, {row[3]} with sqft")

              cursor.execute("SELECT COUNT(*) FROM listings")
              print(f"\nTotal listings: {cursor.fetchone()[0]}")

              cursor.execute("SELECT COUNT(*) FROM listings WHERE size_sqft > 0")
              print(f"With sqft: {cursor.fetchone()[0]}")

              conn.close()
          except Exception as e:
              print(f"Error querying database: {e}")
          EOF

      - name: Show recent scrape runs
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              print("=== Recent Scrape Runs ===")
              cursor.execute("""
                  SELECT run_id, spider_name, status, items_scraped, error_count,
                         ROUND(duration_seconds/60, 1) as minutes
                  FROM scrape_runs
                  ORDER BY started_at DESC
                  LIMIT 10
              """)
              for row in cursor.fetchall():
                  status_icon = "✓" if row[2] == "completed" else "✗"
                  print(f"  {status_icon} {row[0]} | {row[1]}: {row[3]} items, {row[4]} errors ({row[5]}m)")

              conn.close()
          except Exception as e:
              print(f"Error: {e}")
          EOF

      - name: Clean duplicate listings (address similarity)
        run: |
          python3 << 'EOF'
          import os
          import psycopg2
          from difflib import SequenceMatcher
          from collections import defaultdict

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("POSTGRES_URL not set")
              exit(0)

          # Source priority (lower = better, agents have better data)
          SOURCE_PRIORITY = {'savills': 1, 'knightfrank': 2, 'foxtons': 3, 'chestertons': 4, 'rightmove': 5}

          def normalize_addr(addr):
              if not addr: return ''
              addr = addr.lower()
              for word in ['london', 'uk', ',', '.']:
                  addr = addr.replace(word, ' ')
              return ' '.join(addr.split())

          conn = psycopg2.connect(url)
          cur = conn.cursor()

          # Get all active listings
          cur.execute("""
              SELECT id, source, address, postcode, price_pcm, bedrooms, size_sqft,
                     SPLIT_PART(postcode, ' ', 1) as district
              FROM listings WHERE is_active = 1 AND price_pcm > 0 ORDER BY id
          """)

          listings = [{'id': r[0], 'source': r[1], 'address': r[2], 'postcode': r[3],
                       'price_pcm': r[4], 'bedrooms': r[5], 'size_sqft': r[6],
                       'district': r[7], 'norm_addr': normalize_addr(r[2])} for r in cur.fetchall()]

          print(f"Checking {len(listings)} active listings for duplicates...")

          # Union-Find for clustering
          parent = {l['id']: l['id'] for l in listings}
          def find(x):
              if parent[x] != x: parent[x] = find(parent[x])
              return parent[x]
          def union(x, y):
              px, py = find(x), find(y)
              if px != py: parent[px] = py

          # Compare listings (same district, beds, price within 5%, address >70% similar)
          for i, a in enumerate(listings):
              for b in listings[i+1:]:
                  if a['district'] != b['district']: continue
                  if a['bedrooms'] != b['bedrooms']: continue
                  if abs(a['price_pcm'] - b['price_pcm']) > 0.05 * max(a['price_pcm'], b['price_pcm']): continue
                  if SequenceMatcher(None, a['norm_addr'], b['norm_addr']).ratio() > 0.70:
                      union(a['id'], b['id'])

          # Group and identify deletions
          clusters = defaultdict(list)
          for l in listings: clusters[find(l['id'])].append(l)

          to_delete = []
          for members in clusters.values():
              if len(members) < 2: continue
              # Sort by: has sqft, source priority, id
              members.sort(key=lambda x: (0 if x['size_sqft'] and x['size_sqft'] > 0 else 1,
                                          SOURCE_PRIORITY.get(x['source'], 9), x['id']))
              to_delete.extend([m['id'] for m in members[1:]])

          if to_delete:
              placeholders = ','.join(['%s'] * len(to_delete))
              cur.execute(f"DELETE FROM price_history WHERE listing_id IN ({placeholders})", to_delete)
              cur.execute(f"DELETE FROM listings WHERE id IN ({placeholders})", to_delete)
              conn.commit()
              print(f"✅ Removed {len(to_delete)} duplicate listings (address similarity >70%)")
          else:
              print("✅ No duplicates found")

          conn.close()
          EOF

      - name: Train price prediction model (V15)
        run: |
          pip install xgboost scikit-learn optuna pandas numpy
          # Full training for scheduled runs, quick mode for manual tests
          if [ "${{ github.event_name }}" = "schedule" ]; then
            echo "Running FULL model training (Optuna optimization)..."
            python rental_price_models_v15.py 2>&1 | tee model_output.txt
          else
            echo "Running QUICK model training (skip Optuna)..."
            python rental_price_models_v15.py --quick 2>&1 | tee model_output.txt
          fi
        timeout-minutes: 30

      - name: Show model results
        run: |
          python3 << 'EOF'
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              cursor.execute("""
                  SELECT run_id, version, samples_total, r2_score, mae, mape, median_ape
                  FROM model_runs ORDER BY created_at DESC LIMIT 1
              """)
              row = cursor.fetchone()
              if row:
                  print(f"=== Latest Model: {row[0]} ({row[1]}) ===")
                  print(f"  Samples: {row[2]}")
                  print(f"  R²: {row[3]:.4f}")
                  print(f"  MAE: £{row[4]:,.0f}")
                  print(f"  MAPE: {row[5]:.1f}%")
                  print(f"  Median APE: {row[6]:.1f}%")

              conn.close()
          except Exception as e:
              print(f"Error: {e}")
          EOF

      # Note: Negotiation report is now generated dynamically at /report
      # No static files needed - data is fetched fresh from Postgres on each load

      - name: Upload logs artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scrape-logs-${{ github.run_id }}
          path: |
            logs/*.log
            scrape_output.txt
            model_output.txt
          retention-days: 7

      - name: Create summary
        run: |
          echo "## Scrape Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Items scraped:** ${{ steps.scrape.outputs.items_scraped }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Database:** Vercel Postgres" >> $GITHUB_STEP_SUMMARY
          echo "- **Dashboard:** https://dashboard-fawn-nu-59.vercel.app" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python3 << 'EOF' >> $GITHUB_STEP_SUMMARY
          import os
          import psycopg2

          url = os.environ.get('POSTGRES_URL')
          if not url:
              print("Database not configured")
              exit(0)

          try:
              conn = psycopg2.connect(url)
              cursor = conn.cursor()

              print("### Database Stats")
              print("| Source | Total | Active | With sqft |")
              print("|--------|-------|--------|-----------|")

              cursor.execute("""
                  SELECT source, COUNT(*),
                         SUM(CASE WHEN is_active = 1 THEN 1 ELSE 0 END),
                         SUM(CASE WHEN size_sqft > 0 THEN 1 ELSE 0 END)
                  FROM listings GROUP BY source ORDER BY COUNT(*) DESC
              """)
              for row in cursor.fetchall():
                  print(f"| {row[0]} | {row[1]} | {row[2]} | {row[3]} |")

              conn.close()
          except Exception as e:
              print(f"Error: {e}")
          EOF

  notify-failure:
    needs: scrape
    runs-on: ubuntu-latest
    if: failure()
    steps:
      - name: Create issue on failure
        uses: actions/github-script@v7
        with:
          script: |
            const title = `Scrape failed: ${new Date().toISOString().split('T')[0]}`;
            const body = `
            The daily scrape workflow failed.

            **Workflow run:** [${context.runId}](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})

            **Dashboard:** Check https://dashboard-fawn-nu-59.vercel.app for details.

            Please check the logs for details.
            `;

            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'scrape-failure'
            });

            if (issues.data.length === 0) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['scrape-failure', 'automated']
              });
            }
